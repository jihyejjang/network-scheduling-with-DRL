# DDQNì„ í™œìš©í•œ ê°•í™”í•™ìŠµ ê¸°ë°˜ íƒ€ì„ìŠ¬ë¡¯ ìŠ¤ì¼€ì¤„ë§(**Timeslot scheduling using Double Deep Q-Network**)

- ëŒ€í•™ì› ë©ì‹¤ì—ì„œ ì§„í–‰í•œ ê°œì¸ ì—°êµ¬
- ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•œ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì¸ DDQN(Double Deep Q-Network)ë¥¼ í™œìš©í•˜ì—¬, ë„¤íŠ¸ì›Œí¬ íŒ¨í‚·ìŠ¤ì¼€ì¤„ë§ ë¬¸ì œ í•´ê²°ë°©ë²• ì—°êµ¬
    - simpyë¥¼ ì‚¬ìš©í•´ ë„¤íŠ¸ì›Œí¬ ì‹œë®¬ë ˆì´ì…˜ êµ¬í˜„ (packet generation, link, node, packet transmission ,...)
    - tensorflowë¡œ DDQNì„ êµ¬í˜„í•´, ë„¤íŠ¸ì›Œí¬ ìƒí™©ì— ë”°ë¥¸ scheduling actionì„ ìµœì í™”
    - ê¸°ì¡´ ìŠ¤ì¼€ì¤„ë§ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì„ìœ¼ë¡œì„œ, ê°•í™”í•™ìŠµì„ ë„¤íŠ¸ì›Œí¬ì— ë„ì…í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ë³´ì„

## Description


- 2022ë…„ 7ì›”í˜¸ ETRI ê²Œì¬
- IEEE access revision ì§„í–‰ì¤‘

### ğŸ” Abstract

ì¦í­ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ ì–‘ì— ë”°ë¼ ì—„ê²©í•´ì§€ëŠ” requirementë¥¼ ì¶©ì¡±í•˜ê¸° ìœ„í•´ ë”¥ëŸ¬ë‹ì„ ì ì¬ì ì¸ ì†”ë£¨ì…˜ìœ¼ë¡œ ë…¼ì˜ë˜ëŠ” ë‹¨ê³„ì—ì„œ, MDPë¡œ ëª¨ë¸ë§ë˜ê¸°ì— ì¢‹ì€ ë„¤íŠ¸ì›Œí¬ì— ê°•í™”í•™ìŠµì„ ì ìš©í•˜ëŠ” ì—°êµ¬ê°€ ì ì  ë§ì´ ì´ë£¨ì–´ì§€ê³  ìˆìŒ. ì´ì— ë”°ë¼ ë³¸ ì—°êµ¬ì—ì„œëŠ” íƒ€ì„ìŠ¬ë¡¯ ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ í™˜ê²½ì„ Simpyë¡œ êµ¬í˜„í•˜ê³  DDQNìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ë‹¨ìˆœíˆ í•™ìŠµí™˜ê²½ì—ì„œ ë¿ë§Œ ì•„ë‹ˆë¼ í™•ì¥ëœ ë„¤íŠ¸ì›Œí¬ í™˜ê²½ê³¼ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì— ì ì‘í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤Œ

### ğŸ™ Summary

- DDQN ì„ í™œìš©í•˜ì—¬, network ìƒí™©ì— ë”°ë¼ priority queueì—ì„œì˜ Serveë¥¼ ì¡°ì ˆí•¨ìœ¼ë¡œì„œ packet scheduling í•™ìŠµ


- ë¹„êµë¥¼ ìœ„í•œ ê¸°ì¡´ íœ´ë¦¬ìŠ¤í‹± ì•Œê³ ë¦¬ì¦˜
    - SP : í•˜ìœ„ ìš°ì„ ìˆœìœ„ì˜ ì „ì†¡ì´ ë³´ì¥ ë˜ì§€ ì•ŠìŒ
    - WRR : networkì˜ utilizationì— ë”°ë¥¸ weightì¡°ì ˆì´ í•„ìš”í•¨
- ì—°êµ¬ ì§„í–‰
    1. timeslot scheduling simulation with heuristic algorithms : í•™ìŠµì— ì‚¬ìš©í•  timeslot size, flow generation process, network parameter(deadline, generation period,â€¦)ë¥¼ ê³ ì •í•˜ê¸° ìœ„í•´ ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ì–´ëŠ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë³´ì´ëŠ”ì§€ í™•ì¸í•˜ê³ , í•™ìŠµ ê²°ê³¼ì™€ ë¹„êµí•˜ê¸° ìœ„í•œ ì§€í‘œ ìƒì„±
    2. Training agent in a single node environment : output portê°€ 1ê°œì¸ single nodeì—ì„œ priority queue schedulingì„ í•™ìŠµ
      <img width="488" alt="node" src="https://user-images.githubusercontent.com/61912635/190987229-ee093194-1c38-43fa-b9db-0cc7d37b7ce6.png">           
    3. Test in network topology : 8ê°œì˜ flowê°€ ì§€ì •ëœ routeë¡œ ì „ì†¡ë˜ëŠ” ì‹¤ì œ ë„¤íŠ¸ì›Œí¬ì™€ ë¹„ìŠ·í•œ topologyí™˜ê²½ì—ì„œ í•™ìŠµëœ DDQN agentë¥¼ ì ìš©í•˜ì—¬ test simulation ì§„í–‰  
      <img width="366" alt="topology" src="https://user-images.githubusercontent.com/61912635/190987220-1dba8aed-761e-4cf1-9be9-428c42b72cd8.png">

### âœ”ï¸ Result

score (accumulated reward of an episode)ê°€ existing algorithmsë¥¼ ëŠ¥ê°€ â‡’ ê°ê° priorityì˜ packetë“¤ì„ deadlineì•ˆì— ì „ì†¡í•˜ëŠ” ë¹„ìœ¨ì´ DDQN agentê°€ ë” ë†’ìŒ 

- learning curve (In a single node envorinment)  
<img width="488" src="https://user-images.githubusercontent.com/61912635/190987235-9dd59ac4-32d7-4a41-8c80-ac06dcd79dbd.png">  

- Loss    
<img width="488" src="https://user-images.githubusercontent.com/61912635/190987230-bd67261b-99bb-48d7-96a3-eb11e6d8d853.png">  
    
- test result in topology at each scenario  
<img width="488" src="https://user-images.githubusercontent.com/61912635/190987232-cd48bd9f-b6bb-470b-8d64-c750c6ab51f4.png"> 

### ğŸ‘©ğŸ»â€ğŸ’» Development
    

### ğŸ› Â Source code

- ddqn
    - node.py : Packet receive/send, state observing, applying action selected
    - src.py: Flow configuration & generation
    - env.py : Network simulation & training (packet forwarding to destination according to its route by timeslot)
    - test.py : Test the model trained in env.py
    - ddqn.py , agent.py : Conducting DDQN algorithm where saving data <s,a,r,sâ€™> in replay memory , training(model fitting),and action selection, etc,..




Paper
----

[ë¥˜ì§€í˜œ, ê¶Œì£¼í˜ and ì •ì§„ìš°. "DDQNì„ í™œìš©í•œ ê°•í™”í•™ìŠµ ê¸°ë°˜ íƒ€ì„ìŠ¬ë¡¯ ìŠ¤ì¼€ì¤„ë§" í•œêµ­í†µì‹ í•™íšŒë…¼ë¬¸ì§€ 47, no.7 (2022) : 944-952.doi: 10.7840/kics.2022.47.7.944](https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002861752)

